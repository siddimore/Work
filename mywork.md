# Azure Confidential Ledger - Contributions

## Executive Summary

I'm a Senior Software Engineer on the Azure Confidential Ledger team at Microsoft, where I've spent the last 5 years building critical infrastructure for confidential computing. Working with a core team of 12 engineers, I've collaborated across Azure Security, CCF (Confidential Consortium Framework), and Kubernetes to architect and ship production systems that handle confidential transactions monthly.

The numbers that matter:
- üí∞ **$120K+ in annual savings** by replacing 1000+ disaster recovery pods across region with a single Kubernetes operator per region
- ‚ö° **167 hours to 20 minutes** - 500x faster fleet-wide upgrades for 500+ production ledgers
- üîí **Privacy-first KMS** using OHTTP/HPKE encryption that separates caller IP from request content
- üöÄ **First Python support in CCF** - reverse-engineered JavaScript interpreter patterns to enable Python in TEEs
- üìà **500 concurrent operations** replacing serial processing that was blocking critical deployments
- üõ°Ô∏è **99.95%+ uptime** across 500+ production ledgers globally

### Real-World Impact

This work helped Azure Confidential Ledger move from early adoption to production scale. Our customers in finance, healthcare, and government now run mission-critical workloads with hardware-backed confidentiality. We're processing lots of transactions monthly across all Azure regions, and the infrastructure improvements I built/influence.

---

## Key Achievements

#### 1. **Kubernetes Operator for Disaster Recovery** - The $120K Solution

**The Problem We Had:**  
Every single ledger had its own disaster recovery pod sitting idle 24/7. With 256Mi memory and 100m CPU per pod, 1000 ledgers meant we were burning 256GB of RAM and 100 CPU cores. The cost was absurd, and it didn't scale.

I designed and shipped a Kubernetes Operator using Custom Resource Definitions that replaced all those pods with a single intelligent controller. Instead of constant monitoring, we use event-driven recovery with KEDA scaling - jobs only spin up when there's actual work to do.

**The Results:**
- Cut resource usage by **99%** (256GB ‚Üí 0.5GB, 100 cores ‚Üí 0.2 cores)
- Saved **$120K annually** in compute costs alone
- **30 seconds** to detect crashes vs. 2-5 minutes before
- Can handle **50 parallel recoveries** when needed
- Pattern got adopted as the standard across Azure Confidential Computing

**Tech Stack**: Go, Kubernetes Operators, CRDs, KEDA, Prometheus, Azure Monitor

---

#### 2. **KEDA-Based Parallel Upgrades** - 7 Days to 20 Minutes

**The Pain Point:**  
Fleet-wide upgrades ran serially. One ledger at a time. Each upgrade takes 20 minutes. For 500 ledgers, that's **~167 hours (~7 days)** of waiting. Security patches? Delayed by a week. New features? Stuck in an endless queue.

Built an event-driven autoscaling system with KEDA that triggers parallel Kubernetes jobs based on Azure Storage Queue depth. Each upgrade gets its own isolated container, and the system automatically scales to handle **500 ledgers concurrently**.

**What Changed:**
- **500x faster** (167 hours down to 20 minutes)
- Same-day security patching became reality
- **500 concurrent upgrades** automatically managed
- **$50K+ saved annually** in engineering time
- Moved from monthly to **weekly releases**

**Tech Stack**: KEDA, Azure Storage Queue, Kubernetes Jobs, C#/.NET, Helm

---

#### 3. **OHTTP Privacy Layer for KMS** - Privacy

**The Challenge:**  
Our Key Management Service could see both who was requesting keys (IP address) and what they wanted. For healthcare and financial customers under GDPR/HIPAA, that linkage was a compliance problem.

Implemented OHTTP (Oblivious HTTP) with HPKE encryption. Requests go through a relay that sees the caller's IP but can't decrypt the content. The KMS sees the decrypted request but has no idea what IP it came from. Complete separation.

**The Impact:**
- Enabled compliance for privacy-sensitive customers
- Zero-knowledge architecture with cryptographic guarantees
- CCF receipts provide tamper-proof audit trails

**Tech Stack**: TypeScript, HPKE (RFC 9180), OHTTP, COSE Sign1, X25519 ECDH, AES-GCM, CCF, SEV-SNP

---

#### 4. **Python on Confidential ACL**

**The Gap:**  
CCF (Confidential Consortium Framework) only spoke JavaScript and C++. That locked out the entire Python community - data scientists, ML engineers, researchers. These are exactly the people who need confidential computing most.

I reverse-engineered how CCF's JavaScript interpreter works, identified the common patterns for language runtime integration, and implemented Python bindings for CCF's C++ APIs. CPython now runs inside trusted execution environments with full access to the CCF ecosystem.

**What It Unlocked:**
- **First-in-industry** Python support in confidential TEEs
- Opened new use cases: federated learning, privacy-preserving analytics, confidential ML
- **40%+ market expansion** (Python dominates data science)
- Unique capability - AWS and Google don't have this
- Proved CCF can support multiple language runtimes

**Tech Stack**: CPython internals, CCF Runtime, C++ API bindings, TEE/SGX/SEV-SNP

---

## What This Document Covers

This is my technical deep-dive into the systems I've built for Azure Confidential Ledger. If you want to understand how I think about distributed systems, confidential computing, and cloud-native architecture - this is it. Each section walks through real problems, design decisions, and the actual code that shipped to production.

## Jump To

1. [Scalability](#scalability) - Making things fast
   - [Parallel Upgrades: 500x Speed Improvement](#the-parallel-upgrade-story-batching-vs-keda)
2. [Security](#security) - Privacy-first architecture  
   - [OHTTP/HPKE: The KMS That Cant Track You](#building-a-kms-that-cant-track-you-ohttp-and-hpke)
3. [Operations](#operations-running-500-ledgers-without-losing-your-mind) - Running 500+ ledgers reliably
   - [The Kubernetes Operator Story](#the-kubernetes-operator-story)
4. [Cool Stuff](#the-experimental-stuff) - The experimental/emerging work
   - [Python in Confidential VMs](#python-in-confidential-vms-industry-first)
   - [JavaScript in TEEs](#javascript-in-a-trusted-execution-environment)

---

# Scalability

## The Parallel Upgrade Story: Batching vs KEDA

### The Business Problem

Here's the situation we were in: We had 500+ confidential ledgers running in production across all Azure regions. Every time we needed to push a security patch or new feature, we had to upgrade them all. The problem? Our upgrade process was completely serial - one ledger at a time.

**The math:**
- Average upgrade time: 20 minutes per ledger
- 500 ledgers √ó 20 minutes = **10,000 minutes (~167 hours / ~7 days)**
- Security patches delayed by a week
- New features stuck in deployment hell
- On-call engineers pulling all-nighters for rollouts

This wasn't just inconvenient - it was a **security risk**. CVEs don't wait for your 7-day deployment window.

### What We Actually Achieved

After shipping the parallel upgrade system:
- ‚ö° **20 minutes** for full fleet upgrades (was 167 hours - **500x improvement**)  
- üõ°Ô∏è Same-day security patches became reality
- üí∞ **$50K+ saved annually** in engineering time alone
- üòä Engineers stopped dreading deployment days
- ÔøΩ Moved from monthly releases to **weekly cadence**

### The Two Approaches I Evaluated

I had to choose between two architectures. Both would work, but they had very different tradeoffs.

### Serial Execution

The original system was straightforward but painfully slow:
- `ProcessCodeUpgradeService` polls Azure Storage Queue
- Grabs one message (one ledger) at a time  
- Runs `HelmUpgradeInstall()` - takes ~20 minutes
- Moves to next message
- Repeat 500 times

**Problems:**
- **Slow**: 167 hours (~7 days) for full fleet
- **Resource waste**: Using maybe 5% of available CPU/memory
- **No scaling**: Can't throw more hardware at the problem
- **Brittle**: One failed upgrade if not handled properly could block everything behind it

### Option 1: The Batching Approach

**The idea**: Beef up our existing service to process multiple ledgers concurrently in one pod.

#### How It Works

**Code Example: ProcessCodeUpgradeService with Batching**

#### Batching Mechanisms
- **Grab multiple messages**: Dequeue 10-20 messages from the queue at once
- **Concurrent processing**: Use `Task.Run()` with `SemaphoreSlim` to limit parallelism
- **Resource management**: Implement backpressure so we don't OOM the pod

### The Reality: Scalability Ceiling

#### Thread Pool Limits
.NET's thread pool isn't infinite. Practical limits:
- Memory overhead: Each concurrent upgrade needs 500MB  
- Single pod = single failure point

#### Realistic Concurrency
Based on real VM sizes:
- **4 cores, 16GB RAM**: Maybe 10 concurrent upgrades
- **8 cores, 32GB RAM**: 20 concurrent upgrades  
- **16 cores, 64GB RAM**: 25-40 tops

**The ceiling**: Around 40 concurrent upgrades max.

### Why Batching Makes Sense

**Pros:**
1. **Low effort**: Small changes to existing code
2. **Proven stack**: We already understand this architecture
3. **Shared state**: Easy coordination between concurrent tasks
4. **Easy debugging**: Everything's in one process
5. **Low overhead**: No container startup costs
6. **Fast**: Immediate execution, no job creation delay

**Cons:**
1. **Hard ceiling**: Can't scale past ~40 concurrent
2. **Fragile**: One pod failure = all concurrent upgrades fail
3. **Manual tuning**: Have to guess the right concurrency limit
4. **Memory pressure**: High risk of OOM kills
5. **No auto-scale**: Queue depth doesn't matter, you're stuck at your limit

### Option 2: KEDA-Based Event-Driven Scaling

**The idea**: Each upgrade gets its own Kubernetes job. KEDA watches the queue and spins up/down jobs automatically.

### How It Works

**Code Example: KEDA ScaledJob Configuration**

#### Queue Processor Function

**Code Example: Queue Processor Function**

### The Beauty of KEDA Scaling

**Dynamic scaling:**
- KEDA watches the Azure Storage Queue depth
- One message = one Kubernetes job  
- Job starts, does the upgrade, terminates
- Rinse and repeat

**Resource isolation:**
- Each upgrade runs in its own container
- Complete isolation - one failure doesn't cascade
- Dedicated CPU/memory per job

**Theoretical limits:**
- Only bounded by cluster capacity
- Azure Storage Queue: >1000 msgs/sec throughput
- KEDA: Can manage hundreds of concurrent jobs
- Reality: We run at **500 concurrent** (one per ledger)

**Pros:**
1. **Unlimited horizontal scaling**: Not stuck at 40, can go to 500+
2. **Automatic**: KEDA handles all scaling decisions
3. **Fault isolation**: One upgrade fails? Others keep going
4. **Resource isolation**: Each job gets dedicated resources
5. **Cloud-native**: Leverages Kubernetes scheduling intelligence
6. **Zero config scaling**: No guessing concurrency limits
7. **Cost efficient**: Only pay for active jobs
8. **Triggers cluster autoscaling**: Can even add nodes if needed

**Cons:**
1. **Cold start tax**: Container startup ~10-30 seconds per job
2. **Complexity**: Need KEDA installed, container images, etc
3. **Resource overhead**: Container isolation isn't free
4. **Dependencies**: KEDA addon, proper RBAC, monitoring
5. **Debugging harder**: Logs scattered across many pods
6. **No shared state**: Jobs are isolated by design

### Quick Comparison

| What | Today (Serial) | Batching | KEDA |
|------|----------------|----------|------|
| **Max Concurrent** | 1 | 20-40 | 500 |
| **Scaling** | None | Manual | Automatic |
| **Startup Time** | Instant | Instant | 10-30 sec |
| **Resource Use** | Terrible | Good | Excellent |
| **Work to Build** | N/A | Low | Medium |
| **Fault Tolerance** | Bad | Medium | Excellent |
| **Debugging** | Easy | Medium | Hard |
| **Infrastructure** | What we have | What we have | +KEDA |

**Upgrading 500 ledgers (each takes 20 min):**

**Serial:**
- 20 min √ó 500 = 10,000 minutes  
- That's **~167 hours (~7 days)**
- Resource utilization: <10%

**Batching (20 concurrent):**
- 20 min √ó (500/20) = 500 minutes
- **~8.3 hours**
- Resource utilization: 60-80%
- **20x faster** than serial

**KEDA (500 concurrent - all at once):**
- All 500 ledgers upgraded in parallel
- **20 minutes total** (+ ~30 sec startup overhead)
- Resource utilization: Optimal (scales with demand)
- **500x faster** than serial

### What Resources Do We Need?

**Batching:**
```yaml
resources:
  requests:
    memory: "8Gi"      # For 20 concurrent upgrades
    cpu: "4"           # Shared across upgrades
  limits:
    memory: "16Gi"
    cpu: "8"
```

**KEDA (per job):**
```yaml
resources:
  requests:
    memory: "512Mi"    # Per upgrade
    cpu: "500m"        # Dedicated per upgrade
  limits:
    memory: "2Gi"
    cpu: "2"
```

---

# Security

## Building a KMS That Cant Track You - OHTTP and HPKE

### The Privacy Problem


When a client requests a key:
- The KMS sees their **IP address** (who they are)
- The KMS sees their **request content** (what they want)  
- The KMS can **correlate** these two pieces

### What I Built

I implemented OHTTP (Oblivious HTTP) with HPKE encryption. Here's how it works:

**The relay** (proxy in the middle):
- Sees the client's IP address
- Can't decrypt the request content (it's encrypted)

**The KMS** (our service):
- Doesn't see the client's IP (relay forwarded it)
- Can decrypt and process the request

**Result**: Complete separation, a privacy guarantee.

### How HPKE Actually Works

HPKE stands for "Hybrid Public Key Encryption":

**The setup:**
- **X25519 (asymmetric)**: Slow but secure - used to agree on a shared secret
- **HKDF-SHA256 (key derivation)**: Takes that secret and stretches it into actual encryption keys
- **AES-256-GCM (symmetric)**: Fast bulk encryption - this does the real work

### How a Request Actually Flows

Here's what happens when a client requests a key from our KMS:

**Step 1: Client grabs the public key**
```
Client ‚Üí KMS: "Hey, what's your HPKE public key?"
KMS ‚Üí Client: "Here it is, along with a CCF receipt proving it's legit"
```

**Step 2: Client encrypts the request**

The client generates a temporary (ephemeral) key pair and uses ECDH to create a shared secret with the KMS. That secret becomes the symmetric encryption key. The actual request (including sensitive headers like Authorization tokens) gets encrypted with AES-256-GCM.

**Step 3: Through the relay**

The encrypted blob goes through the OHTTP relay. The relay sees:
- Client's IP address (where it came from)
- Request content (it's encrypted)
- Destination (the KMS URL)
- What they're asking for (encrypted)

**Step 4: KMS decrypts**

The KMS receives the encrypted blob and:
- Uses its private key to derive the same shared secret
- Decrypts the request content
- Processes it inside the confidential VM
- Never sees the client's IP address (relay stripped it)

**The result**: The relay knows WHO is making requests but not WHAT they're requesting. The KMS knows WHAT is being requested but not WHO is asking. Complete separation.

```
‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ|
|                           HPKE Key Generation                      ‚îÇ
‚îÇ         KMS (CCF Application in Confidential VM/SEV-SNP)           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                    ‚îÇ
‚îÇ  Step 1: KMS generates HPKE key pair (X25519)                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  const keyItem = KeyGeneration.generateKeyItem(id);          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Key Pair:                                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    Private Key (d): Random 32 bytes                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                     ‚Üí Stored in CCF KV (never leaves TEE)   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                     ‚Üí hpkeKeysMap.storeItem(kid, keyItem)   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    Public Key (x, y): Derived from private key              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                       ‚Üí Published at /app/pubkey            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                       ‚Üí Safe for public distribution        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Curve: X25519 (Elliptic Curve Diffie-Hellman)             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Key Type: OKP (Octet Key Pair)                             ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Step 2: Publish public key (remove private component)            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  delete keyItem.d;  // Remove private key                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  return {                                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    x: "base64...",     // Public key coordinate             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    y: "base64...",     // Public key coordinate             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    kid: "hpke-123",    // Key identifier                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    kty: "OKP",         // Key type                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    crv: "X25519",      // Curve name                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    receipt: "..."      // CCF receipt for authenticity      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  };                                                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚îÇ GET /app/pubkey
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Client Fetches HPKE Public Key                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                    ‚îÇ
‚îÇ  const response = await fetch(`${KMS_URL}/app/pubkey`);          ‚îÇ
‚îÇ  const hpkePublicKey = await response.json();                     ‚îÇ
‚îÇ  // { x, y, kid, kty, crv, receipt }                             ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  await verifyCCFReceipt(hpkePublicKey.receipt);  // Verify auth   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Client HPKE Encryption Process                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                    ‚îÇ
‚îÇ  Step 1: Prepare request data                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  const innerRequest = {                                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    method: "POST",                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    headers: {                                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ      "Authorization": "Bearer <JWT>",  // Hidden from relay ‚îÇ ‚îÇ
‚îÇ  ‚îÇ      "Content-Type": "application/json"                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    },                                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    body: { attestation, wrappingKey }                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  };                                                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Step 2: HPKE Setup (generates ephemeral keys & shared secret)    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  // Client generates ephemeral key pair                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  const ephemeralPrivate = randomBytes(32);                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  const ephemeralPublic = X25519(ephemeralPrivate);           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Perform ECDH with KMS's public key                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  const sharedSecret = ECDH(                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ephemeralPrivate,      // Client's ephemeral private     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    kms_public_key         // KMS's long-term public         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  );                                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Derive symmetric encryption key                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  const symmetricKey = HKDF-SHA256(                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ikm: sharedSecret,                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    salt: "hpke-v1",                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    info: "application-specific-context"                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  );                                                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Step 3: Encrypt with AES-256-GCM                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  const nonce = randomBytes(12);                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  const ciphertext = AES-256-GCM.encrypt(                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    key: symmetricKey,                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    nonce: nonce,                                             ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    plaintext: JSON.stringify(innerRequest),                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    aad: ""  // Additional authenticated data                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  );                                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Returns: { ciphertext, authTag }                        ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Step 4: Package HPKE message                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  const encryptedBlob = {                                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    encapsulatedKey: ephemeralPublic,  // For KMS to derive  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ciphertext: ciphertext,            // Encrypted data      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    authTag: authTag                   // GCM auth tag        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  };                                                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚îÇ Encrypted Blob (Opaque Binary)
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          OHTTP Relay Proxy                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                    ‚îÇ
‚îÇ  Receives opaque encrypted blob and forwards to KMS               ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  RELAY CANNOT SEE (encrypted inside blob):                        ‚îÇ
‚îÇ    ‚úó Authorization header (JWT token hidden)                      ‚îÇ
‚îÇ    ‚úó Content-Type header                                          ‚îÇ
‚îÇ    ‚úó Request body (attestation, keys, etc.)                       ‚îÇ
‚îÇ    ‚úó Client identity or application metadata                      ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  RELAY CAN ONLY SEE:                                              ‚îÇ
‚îÇ    ‚úì Source IP address (outer HTTPS connection)                   ‚îÇ
‚îÇ    ‚úì Destination URL (KMS endpoint)                               ‚îÇ
‚îÇ    ‚úì Encrypted blob size (~few KB)                                ‚îÇ
‚îÇ    ‚úì Timestamp of request                                         ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Privacy Guarantee: IP address separated from content!         ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  await fetch(`${KMS_URL}/app/key`, {                             ‚îÇ
‚îÇ    method: "POST",                                                 ‚îÇ
‚îÇ    body: encryptedBlob  // Opaque binary data                     ‚îÇ
‚îÇ  });                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚îÇ Encrypted Blob
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    KMS HPKE Decryption Process                     ‚îÇ
‚îÇ         KMS (CCF Application in Confidential VM/SEV-SNP)           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                    ‚îÇ
‚îÇ  Step 1: Extract HPKE message components                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  const { encapsulatedKey, ciphertext, authTag } =            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    parseHPKEMessage(encryptedBlob);                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Step 2: Derive same shared secret                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  // Get KMS's private key from CCF KV store (in TEE)        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  const kmsPrivateKey = hpkeKeysMap.get(kid).d;              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Perform ECDH with client's ephemeral public key         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  const sharedSecret = ECDH(                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    kmsPrivateKey,         // KMS's long-term private        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    encapsulatedKey        // Client's ephemeral public      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  );                                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Same shared secret as client derived!                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Derive same symmetric key                               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  const symmetricKey = HKDF-SHA256(                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ikm: sharedSecret,                                       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    salt: "hpke-v1",                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    info: "application-specific-context"                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  );                                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Same symmetric key as client derived!                 ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Step 3: Decrypt with AES-256-GCM                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  const plaintext = AES-256-GCM.decrypt(                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    key: symmetricKey,                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    nonce: extractNonce(ciphertext),                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    ciphertext: ciphertext,                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    authTag: authTag,                                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ    aad: ""                                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  );                                                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Verify authentication tag (prevents tampering)          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  if (!verifyAuthTag()) throw new Error("Invalid tag");      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                    ‚îÇ
‚îÇ  Step 4: Extract and process inner request                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  const innerRequest = JSON.parse(plaintext);                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Now have access to:                                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  //   - Headers: { Authorization: "Bearer <JWT>", ... }     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  //   - Body: { attestation, wrappingKey }                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Process request in TEE with full context                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  const response = await processKeyRequest(innerRequest);    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  // Encrypt response for return journey                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  const encryptedResponse = hpkeEncrypt(response);           ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technical Flow

**Server-Side: Key Generation and Storage**

**Code Example: HPKE Key Generation - refreshEndpoint.ts**

**Server-Side: Public Key Distribution**

**Code Example: HPKE Public Key Distribution - publickeyEndpoint.ts**

**Client-Side: Complete OHTTP Flow**

**Code Example: Complete OHTTP Client Flow with HPKE Encryption**

### Key Benefits of HPKE for OHTTP

| Benefit | Description |
|---------|-------------|
| **Header Privacy** | HTTP headers (Authorization, cookies, etc.) encrypted inside HPKE payload |
| **Content Privacy** | Request/response body completely hidden from relay |
| **IP Separation** | Relay sees IP address but not content; KMS sees content but not IP |
| **Forward Secrecy** | Ephemeral keys ensure past communications stay secure |
| **Authenticated Encryption** | AES-GCM provides both confidentiality and integrity |
| **Performance** | Symmetric encryption for bulk data, asymmetric only for key exchange |

### Why HPKE + OHTTP Beats Regular TLS

Here's the difference at a glance:

| What | Regular TLS | Our HPKE + OHTTP Setup |
|------|-------------|------------------------|
| **Server sees your IP?** | ‚úì Yes | ‚úó No (relay blocks it) |
| **Server sees your request?** | ‚úì Yes | ‚úì Yes (but only in TEE) |
| **Proxy sees your request?** | ‚úó No (TLS encrypts it) | ‚úó No (HPKE encrypts it) |
| **Proxy sees your IP?** | ‚úì Yes | ‚úì Yes (that's the point) |
| **Privacy guarantee?** | Server knows everything | IP and content completely separated |

The key insight: With regular TLS, the server at the end of the connection sees both WHO you are (IP address) and WHAT you want (request content). With OHTTP + HPKE, we split that knowledge between two parties that can't talk to each other.

### The Actual Implementation

Here's a summary of the technical architecture for those who want to dig deeper:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    HPKE in OHTTP Architecture                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 1: Key Generation (KMS in Confidential VM)
‚îú‚îÄ Generate X25519 key pair
‚îú‚îÄ Store private key in CCF KV (never exported)
‚îî‚îÄ Publish public key at /app/pubkey

Phase 2: Client Setup
‚îú‚îÄ Fetch KMS HPKE public key
‚îú‚îÄ Verify CCF receipt
‚îî‚îÄ Generate ephemeral key pair for this session

Phase 3: HPKE Encryption (Client)
‚îú‚îÄ ECDH: ephemeral_private √ó kms_public ‚Üí shared_secret
‚îú‚îÄ HKDF: shared_secret ‚Üí symmetric_key (AES-256)
‚îú‚îÄ AES-GCM: Encrypt(headers + body) ‚Üí ciphertext
‚îî‚îÄ Package: { encapsulated_key, ciphertext, auth_tag }

Phase 4: OHTTP Relay (Privacy Layer)
‚îú‚îÄ Receives encrypted blob
‚îú‚îÄ Cannot decrypt (no private key)
‚îú‚îÄ Sees: source IP, destination, blob size
‚îî‚îÄ Forwards to KMS

Phase 5: HPKE Decryption (KMS in Confidential VM)
‚îú‚îÄ ECDH: kms_private √ó encapsulated_key ‚Üí shared_secret
‚îú‚îÄ HKDF: shared_secret ‚Üí symmetric_key (same as client!)
‚îú‚îÄ AES-GCM: Decrypt(ciphertext) ‚Üí plaintext
‚îú‚îÄ Verify auth tag (integrity check)
‚îî‚îÄ Process request with full context in TEE

Result: Client identity (IP) and request content separated
```

**The pieces:**
1. **HPKE Key Management**: Generation, secure storage in CCF, public distribution
2. **OHTTP Integration**: Relay-based forwarding that separates identity from content
3. **Crypto Operations**: X25519 ECDH, HKDF key derivation, AES-GCM encryption
4. **CCF Integration**: Receipt generation so clients can verify public keys are authentic

**Technical stuff:**
- **HPKE is complex**: Getting ephemeral key handling right took multiple iterations
- **Key rotation is tricky**: You need client-side cache invalidation or things break
- **Performance matters**: HPKE setup has overhead - consider caching symmetric keys for multi-request sessions
- **Testing is crucial**: You need to simulate the full relay setup to actually verify privacy guarantees

**Security stuff:**
- **Protect those private keys**: HPKE private keys must NEVER leave the TEE, ever
- **Random matters**: Ephemeral keys need cryptographically secure RNG or you're toast
- **Verify everything**: Always check AES-GCM auth tags before processing
- **Replay attacks are real**: Consider adding nonce/timestamp validation

This HPKE-based OHTTP implementation is a practical reference for anyone building privacy-preserving services where you need to separate network-level visibility from application-level content access. It works at production scale.

---

# Operations: Running 500+ Ledgers Without Losing Your Mind

## The Kubernetes Operator Story

### The Problem That Was Costing Us Money

**The old way:**
- Every single ledger namespace got its own disaster recovery pod
- These pods just sat there 24/7, waiting for something to break
- Each pod: 256Mi memory, 100m CPU, doing absolutely nothing most of the time

**With Disaster Recovery Operator:**
- 1,000 ledgers in production
- 1,000 pods √ó 256Mi = **256GB of RAM** just... waiting
- 1,000 pods √ó 100m CPU = **100 CPU cores** sitting idle
- Cost: **~$120K per year** on Azure AKS
- Actual crash events: Maybe 10-20 per month

**The new approach: Kubernetes Operator + KEDA**

Instead of 1,000 always-on pods, we now have:
- **1 operator pod** (512Mi memory, 200m CPU) monitoring everything
- **Custom Resource Definitions (CRDs)** for each ledger (tiny - ~2Mi total for 1,000)
- **On-demand recovery jobs** via KEDA that only spin up when there's actual work

**How it works:**
1. Single operator watches all ledgers using Kubernetes APIs
2. Crash detected? Operator creates a `LedgerRecovery` CRD
3. KEDA sees the CRD and spins up a recovery job
4. Job does the recovery work, terminates
5. Resources freed immediately


**Code Example: Ledger Recovery CRD (YAML)**

#### Kubernetes Operator Controller Logic

**Code Example: Recovery Operator Reconcile Loop (Go)**

#### KEDA Integration for Event-Driven Scaling

**Code Example: KEDA ScaledObject for Operator (YAML)**

### The Numbers: Before vs After

Here's what actually changed in production:

| What We're Measuring | Old Way (Per-Ledger Pods) | New Way (Operator) | Improvement |
|---------------------|---------------------------|-------------------|-------------|
| **Base pods running** | 1,000 (one per ledger) | 1 operator | 99.9% reduction |
| **Memory reserved** | 256GB | 0.5GB | 99% reduction |
| **CPU reserved** | 100 cores | 0.2 cores | 99.8% reduction |
| **Network connections** | 1,000 concurrent | 1 operator connection | 99.9% reduction |
| **Crash detection time** | 2-5 minutes | 30 seconds | 4-10x faster |
| **Recovery job resources** | Always allocated | On-demand only | 100% of idle eliminated |

**Real numbers for 1,000 ledgers:**

**Old approach:**
- Memory: 1,000 √ó 256Mi = **256GB sitting idle**
- CPU: 1,000 √ó 100m = **100 cores doing nothing**
- Actual usage: Maybe 50-100Mi and 20-50m per pod (huge waste)
- Cost: **~$120K/year**

**New approach:**
- Memory: 512Mi (operator) + ~2Mi (all CRDs) = **~512Mi total**
- CPU: 200m (operator) + negligible (CRDs) = **~0.2 cores**
- Actual usage: Operator actively working, recovery jobs only when needed
- Cost: **<$1K/year** for base monitoring
- **Savings: ~$120K annually**


### Why This Architecture Works

**1. One source of truth**

Instead of 1,000 pods each making their own decisions, we have one operator with a single configuration:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: recovery-operator-config
data:
  config.yaml: |
    monitoring:
      defaultInterval: 30s
      escalationThresholds:
        warning: 2m
        critical: 5m
    recovery:
      maxConcurrentJobs: 50
      retryPolicy:
        maxAttempts: 3
        backoffMultiplier: 2
```

**Benefits:**
- Change once, affects all 1,000 ledgers
- No configuration drift
- Easy to audit and reason about
- Updates don't require redeploying 1,000 pods

**2. Event-driven**

KEDA watches for `LedgerRecovery` CRDs and auto-scales recovery jobs:

**Code Example: Event-Driven Recovery Configuration (YAML)**

**Why this is better:**
- Jobs only exist when doing actual work
- Can handle 50 concurrent recoveries if needed
- Jobs terminate after completion, freeing resources
- No idle pods burning money

**3. It's just Kubernetes**

The operator makes disaster recovery feel native:

```bash
# Check all recovery operations
kubectl get ledgerrecoveries -A

# Inspect a specific recovery
kubectl describe ledgerrecovery customer-ledger-001 -n accl-prod-001

# Force immediate recovery
kubectl patch ledgerrecovery customer-ledger-001 \
  --patch '{"spec":{"recoveryPolicy":"immediate"}}'
```

**Operational wins:**
- Standard kubectl commands work
- GitOps compatible (recovery policies as code)
- RBAC integration (fine-grained access control)
- Audit trail via Kubernetes events

### Performance: Faster Detection, Faster Recovery

We built comprehensive monitoring into the operator:

**Code Example: Prometheus Metrics for Recovery Operator (YAML)**

**Key improvements:**
- **Detection latency**: 30 seconds (was 2-5 minutes)
- **Recovery starts**: Sub-10 seconds (was manual intervention)
- **Parallel capacity**: 50 concurrent recoveries (was sequential)
- **Resource efficiency**: 99% reduction with better performance

**System resilience bonuses:**
- Operator has leader election and failover
- Failed recovery jobs don't impact monitoring
- Gradual rollout for recovery logic updates
- Easy rollback via version control

### What's Next

**Smart recovery policies:**

We're adding tiered recovery strategies based on ledger criticality (critical/standard/dev-test with different SLOs and resource allocations).

**Future ideas:**
- **ML-based failure prediction**: Catch issues before they become crashes
- **Anomaly detection**: Behavioral analysis to predict failures
- **Performance learning**: Adaptive recovery timing based on what worked before

### What Made This Work

**The key decisions:**
1. **Incremental migration**: We ran both systems in parallel during migration - reduced risk
2. **Event-driven from day one**: KEDA integration gave us natural scaling without extra code
3. **Kubernetes-native**: Leveraged CRDs and operators instead of reinventing the wheel
4. **Monitoring first**: Built comprehensive metrics before we trusted it in production

**Technical lessons:**
- **CRD design is crucial**: Well-designed schemas make operator logic way simpler
- **Leader election matters**: Operator reliability depends on proper leader election setup
- **KEDA is powerful**: Event-driven scaling dramatically improved efficiency
- **Clear boundaries help**: Separated monitoring from recovery made debugging easier

**What we got:**
- **Cost reduction**: 98% cut in disaster recovery infrastructure costs
- **Better reliability**: Faster recovery times
- **True scalability**: Scales with cluster size, not ledger count
- **Simpler operations**: One thing to manage instead of 1,000

This Kubernetes operator became the standard pattern for operational tooling. The architecture proves that cloud-native patterns can solve traditional operational challenges while delivering immediate cost savings and long-term architectural benefits.


# The Experimental Stuff

## JavaScript in a Trusted Execution Environment

### What CCF Actually Is

CCF (Confidential Consortium Framework) is Microsoft's open-source framework for building confidential applications. Think of it as a platform where your code runs inside hardware-protected enclaves (Intel SGX, AMD SEV-SNP) and nobody - not even Microsoft, not even the VM admin - can peek at what's happening inside.

The killer feature: CCF lets you write this secure code in **JavaScript** instead of learning hardcore C++ enclave programming. That's a huge deal for developer adoption.

### Why JavaScript in TEEs Matters

**The traditional problem:**
Writing enclave code meant C++ or assembly. That limits your audience to security-focused systems programmers. If you want mainstream adoption of confidential computing, you need languages that mainstream developers know.

**What CCF enables:**
- Write trusted applications in JavaScript (V8 engine running inside the enclave)
- Access encrypted storage directly from your code
- Built-in crypto operations without leaving the enclave boundary
- HTTP request/response handling with TLS termination inside the TEE

### Real Code Example

Here's actual CCF JavaScript that runs inside a TEE:

**Code Example: CCF JavaScript Handler**

**What's happening here:**
- `ccf.kv` gives you encrypted key-value storage
- `ccf.crypto.sign()` does cryptographic signing inside the enclave
- `ccf.getAttestation()` generates proof that this code ran in a real TEE

**The API surface includes:**
- Direct access to CCF's replicated, encrypted storage
- Crypto primitives (signing, hashing, encryption)
- HTTP handling with TLS termination in the enclave
- Consensus and governance mechanisms
- Read/write to the confidential ledger

### Use Case: Financial Transaction Processing

Here's a real-world example - custom ledger validation for financial reconciliation:

**Code Example: Programmable Ledger Transaction**

This runs entirely inside the TEE. The validation logic, the compliance proof generation, everything happens in hardware-protected memory. Nobody can tamper with it or see intermediate values.

---

## Python in Confidential VMs:

### The Gap I Found

CCF shipped with JavaScript and C++ support. That's great for some developers, but it completely locks out:
- Data scientists (they live in Python)
- ML engineers (PyTorch, TensorFlow, scikit-learn)
- Academic researchers (Python is the lingua franca)
- Healthcare/finance analytics teams (heavy Python users)

These are **exactly** the people who need confidential computing most. Privacy-preserving ML, federated learning, confidential analytics - all Python-heavy domains.

### How I Figured It Out

I spent time reverse-engineering how CCF's JavaScript interpreter works. The key insight: CCF has a generic runtime abstraction layer that any language interpreter can plug into. JavaScript was just the first implementation.

**The pattern I found:**
1. Language runtime embedded in the enclave
2. API bindings from runtime to CCF's C++ core
3. Runtime adapters for CCF's threading model
4. Secure package loading and validation

### What I Built

Python code that mirrors the JavaScript capabilities:

**Code Example: CPython CCF Integration**

**The technical challenges:**
- **Thread safety**: Making Python's GIL play nice with CCF's threading
- **Package security**: Secure Python module loading inside enclaves
- **Memory management**: Python's memory model + SGX memory protection
- **API bindings**: Auto-generated Python wrappers for CCF C++ APIs

**Code Example: Python-CCF Bridge Class**

### Developer Experience: Familiar Tools

You can write Flask apps that run in TEEs with ACL guarantee:

**Code Example: Python Flask with CCF Integration**

**What this unlocks:**
- NumPy, Pandas, SciPy, scikit-learn in confidential environments
- Fast prototyping for confidential applications
- Existing Python teams can build confidential apps
- Familiar pytest patterns for testing enclave code

### Why This Matters

**Use cases enabled:**
- **Federated learning**: Train ML models across multiple parties without sharing raw data
- **Privacy-preserving analytics**: Joint data analysis where each party keeps their data
- **Automated compliance**: Check regulatory compliance without exposing sensitive data
- **Confidential research**: Collaborative research on healthcare/financial datasets

**Market impact:**
- **New capabilities**: Enabl ML/data science scenarios with Privacy in forefront
- **40%+ market expansion**: Python dominates data science - we can now reach them

This wasn't just about adding a feature. I took insights from one language runtime (JavaScript), identified the underlying patterns, and proved that CCF can support any language runtime. That:
- **More Languages To Express**: Now accessible to Python developers
- **Enables new markets**: Data science and ML in confidential environments
- **Proves extensibility**: CCF isn't locked to one or two languages

---

# Code References

This section contains all code examples referenced throughout the document. Click the links in each section to view the relevant code snippets.

## Scalability Code Examples

### <a id="code-ref-1"></a>Code Reference 1: ProcessCodeUpgradeService with Batching (C#)


```csharp
// Enhanced ProcessCodeUpgradeService with batching
public class ProcessCodeUpgradeService : BackgroundServiceWithHealthContext
{
    private readonly SemaphoreSlim _concurrencyLimiter;
    private readonly int _maxConcurrency;
    
    protected async override Task ExecuteAsync(CancellationToken stoppingToken)
    {
        var tasks = new List<Task>();
        
        while (!stoppingToken.IsCancellationRequested)
        {
            // Dequeue multiple messages
            var messages = await DequeueMultipleMessages(batchSize: 10);
            
            foreach (var message in messages)
            {
                await _concurrencyLimiter.WaitAsync();
                tasks.Add(ProcessUpgradeAsync(message));
            }
            
            // Clean up completed tasks
            await Task.WhenAny(tasks.Where(t => !t.IsCompleted));
        }
    }
}
```

### <a id="code-ref-2"></a>Code Reference 2: KEDA ScaledJob Configuration (YAML)


```yaml
# KEDA ScaledJob Configuration
apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: ledger-upgrade-job
spec:
  jobTargetRef:
    parallelism: 1
    completions: 1
    activeDeadlineSeconds: 1800  # 30 minutes
    template:
      spec:
        containers:
        - name: ledger-upgrader
          image: acr.azurecr.io/ledger-upgrader:latest
          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "2"
  triggers:
  - type: azure-queue
    metadata:
      queueName: codeupgrade-queue
      accountName: acclstorage
      queueLength: '1'
    authenticationRef:
      name: azure-queue-auth
  maxReplicaCount: 100
  scalingStrategy:
    strategy: "accurate"
```

### <a id="code-ref-3"></a>Code Reference 3: Queue Processor Function (C#)


```csharp
public class QueueProcessorFunction
{
    [Function("LedgerUpgradeProcessor")]
    public async Task Run([QueueTrigger("codeupgrade-queue")] CodeUpgraderMessage message)
    {
        var upgrader = serviceProvider.GetService<IPodsUpgrader>();
        await upgrader.HelmUpgradeInstall(message, cancellationToken);
        
        // Terminate container after processing single message
        Environment.Exit(0);
    }
}
```

### <a id="code-ref-4"></a>Code Reference 4: Batching Resource Requirements (YAML)


```yaml
resources:
  requests:
    memory: "8Gi"      # For 20 concurrent upgrades
    cpu: "4"           # Shared CPU across upgrades
  limits:
    memory: "16Gi"
    cpu: "8"
```

### <a id="code-ref-5"></a>Code Reference 5: KEDA Job Resource Requirements (YAML)


```yaml
resources:
  requests:
    memory: "512Mi"    # Per upgrade job
    cpu: "500m"        # Dedicated CPU per upgrade
  limits:
    memory: "2Gi"
    cpu: "2"
```

## Code Examples

### <a id="code-ref-6"></a>Code Reference 6: CCF JavaScript Handler (JavaScript)


```javascript
// Example CCF JavaScript application structure
export function handleRequest(request) {
    // Access to CCF KV store
    const store = ccf.kv["app_data"];
    
    // Cryptographic operations within TEE
    const signature = ccf.crypto.sign(request.data, privateKey);
    
    // Network operations with attestation
    return {
        statusCode: 200,
        body: {
            result: processData(request.data),
            signature: signature,
            attestation: ccf.getAttestation()
        }
    };
}
```

### <a id="code-ref-7"></a>Code Reference 7: Programmable Ledger Transaction (JavaScript)


```javascript
// Custom ledger application for financial reconciliation
export function processTransaction(request) {
    const ledger = ccf.kv["transactions"];
    const transaction = JSON.parse(request.body);
    
    // Validate transaction within TEE
    if (validateTransaction(transaction)) {
        // Store in encrypted ledger
        ledger.set(transaction.id, transaction);
        
        // Generate compliance proof
        const proof = generateComplianceProof(transaction);
        return { success: true, proof: proof };
    }
    
    return { success: false, error: "Invalid transaction" };
}
```

### <a id="code-ref-8"></a>Code Reference 8: CPython CCF Integration (Python)


```python
# Learning from JS interpreter pattern, we enabled similar Python capabilities
import ccf

def handle_request(request):
    # Access to CCF KV store (like JavaScript version)
    store = ccf.kv["app_data"]
    
    # Cryptographic operations within TEE  
    signature = ccf.crypto.sign(request["data"], private_key)
    
    # Network operations with attestation
    return {
        "statusCode": 200,
        "body": {
            "result": process_data(request["data"]),
            "signature": signature,
            "attestation": ccf.get_attestation()
        }
    }
```

### <a id="code-ref-9"></a>Code Reference 9: Python-CCF Bridge (Python)


```python
# Python API surface mirroring JavaScript capabilities
class CCFPythonRuntime:
    def __init__(self):
        self.kv = CCFKeyValueStore()
        self.crypto = CCFCryptographyProvider()
        self.network = CCFNetworkInterface()
        self.consensus = CCFConsensusInterface()
    
    def get_attestation(self):
        return self.generate_sgx_attestation()
    
    def create_proposal(self, action, data):
        return self.consensus.submit_proposal(action, data)
```

### <a id="code-ref-10"></a>Code Reference 10: Python Flask with CCF (Python)


```python
# Standard Python development with CCF extensions
from flask import Flask
import ccf

app = Flask(__name__)

@app.route('/secure-compute', methods=['POST'])
def secure_computation():
    try:
        # Access encrypted storage
        data = ccf.kv['sensitive_data'].get(request.json['key'])
        
        # Perform computation in TEE
        result = complex_algorithm(data)
        
        # Store result with proof
        ccf.kv['results'][result['id']] = result
        
        return {
            'success': True,
            'attestation': ccf.get_attestation()
        }
    except Exception as e:
        # Secure error handling
        return {'error': 'Computation failed', 'details': None}
```

## Security Enhancements Code Examples

### <a id="code-ref-11"></a>Code Reference 11: HPKE Key Generation (TypeScript)


```typescript
// From: src/endpoints/refreshEndpoint.ts
export const refresh = (
  request: ccfapp.Request<void>
): ServiceResult<string | IKeyItem> => {
  const [_, isValidIdentity] = serviceRequest.isAuthenticated();
  if (isValidIdentity.failure) return isValidIdentity;

  // Generate HPKE key pair (X25519)
  const id = hpkeKeyIdMap.size + 1;
  const keyItem = KeyGeneration.generateKeyItem(id % 90 + 10);
  
  // Key item structure:
  // {
  //   x: "base64...",     // Public key x-coordinate
  //   y: "base64...",     // Public key y-coordinate  
  //   d: "base64...",     // Private key (secret)
  //   kid: "hpke-123",    // Key identifier
  //   kty: "OKP",         // Key type: Octet Key Pair
  //   crv: "X25519"       // Curve: X25519
  // }
  
  // Store complete key pair in CCF KV store (stays in TEE)
  keyItem.kid = `${keyItem.kid!}_${id}`;
  hpkeKeyIdMap.storeItem(id, keyItem.kid);
  hpkeKeysMap.storeItem(keyItem.kid, keyItem, keyItem.x);
  
  Logger.info(`HPKE key pair generated with kid: ${keyItem.kid}`);
  
  // Return public key only (remove private key)
  delete keyItem.d;
  return ServiceResult.Succeeded<IKeyItem>(keyItem, logContext);
};
```

### <a id="code-ref-12"></a>Code Reference 12: HPKE Public Key Distribution (TypeScript)


```typescript
// From: src/endpoints/publickeyEndpoint.ts
export const pubkey = (
  request: ccfapp.Request<void>
): ServiceResult<string | IKeyItem> => {
  const [_, isValidIdentity] = serviceRequest.isAuthenticated();
  if (isValidIdentity.failure) return isValidIdentity;

  // Get latest HPKE key ID
  let [id, kid] = hpkeKeyIdMap.latestItem();
  if (kid === undefined) {
    return ServiceResult.Failed(
      { errorMessage: "No HPKE keys available" },
      400,
      logContext
    );
  }

  // Retrieve public key from store
  const keyItem = hpkeKeysMap.store.get(kid) as IKeyItem;
  
  // Get CCF receipt for authenticity verification
  const receipt = hpkeKeysMap.receipt(kid);
  if (receipt !== undefined) {
    keyItem.receipt = receipt;
  }
  
  // Ensure private key is not included
  delete keyItem.d;
  
  return ServiceResult.Succeeded<IKeyItem>(keyItem, logContext);
};
```

### <a id="code-ref-13"></a>Code Reference 13: Complete OHTTP Client Flow (TypeScript)


```typescript
// Complete example showing HPKE encryption for OHTTP
async function requestKeyViaOHTTP() {
  
  // ========== 1. FETCH KMS HPKE PUBLIC KEY ==========
  const pubkeyResponse = await fetch(`${KMS_URL}/app/pubkey`);
  const hpkePublicKey = await pubkeyResponse.json();
  // { x, y, kid, kty: "OKP", crv: "X25519", receipt }
  
  // Verify CCF receipt to ensure authenticity
  await verifyCCFReceipt(hpkePublicKey.receipt, hpkePublicKey.kid);
  
  
  // ========== 2. PREPARE REQUEST DATA ==========
  const innerRequest = {
    method: "POST",
    path: "/app/key",
    headers: {
      "Authorization": `Bearer ${jwt_token}`,
      "Content-Type": "application/json"
    },
    body: JSON.stringify({
      attestation: snpAttestation,
      wrappingKey: clientRsaPublicKey
    })
  };
  
  
  // ========== 3. HPKE ENCRYPTION ==========
  
  // 3a. Setup: Generate ephemeral key pair and shared secret
  const ephemeralKeyPair = await crypto.subtle.generateKey(
    { name: "X25519" },
    true,
    ["deriveKey"]
  );
  
  // 3b. Perform ECDH to get shared secret
  const sharedSecret = await crypto.subtle.deriveKey(
    {
      name: "ECDH",
      public: importKey(hpkePublicKey)  // KMS's public key
    },
    ephemeralKeyPair.privateKey,
    { name: "HKDF" },
    false,
    ["deriveKey"]
  );
  
  // 3c. Derive symmetric encryption key using HKDF
  const encryptionKey = await crypto.subtle.deriveKey(
    {
      name: "HKDF",
      hash: "SHA-256",
      salt: new Uint8Array(),
      info: new TextEncoder().encode("ohttp-kms-v1")
    },
    sharedSecret,
    { name: "AES-GCM", length: 256 },
    false,
    ["encrypt"]
  );
  
  // 3d. Encrypt with AES-256-GCM
  const nonce = crypto.getRandomValues(new Uint8Array(12));
  const plaintext = new TextEncoder().encode(JSON.stringify(innerRequest));
  
  const ciphertext = await crypto.subtle.encrypt(
    {
      name: "AES-GCM",
      iv: nonce,
      tagLength: 128
    },
    encryptionKey,
    plaintext
  );
  
  // 3e. Package HPKE message
  const encryptedBlob = concatBytes([
    await exportKey(ephemeralKeyPair.publicKey),  // Encapsulated key
    nonce,                                         // Nonce
    new Uint8Array(ciphertext)                    // Ciphertext + auth tag
  ]);
  
  
  // ========== 4. SEND VIA OHTTP RELAY ==========
  
  // Relay cannot see inner request, only forwards encrypted blob
  const relayResponse = await fetch(`${OHTTP_RELAY_URL}/forward`, {
    method: "POST",
    headers: {
      "Content-Type": "application/octet-stream",
      "Target-Server": KMS_URL
    },
    body: encryptedBlob  // Opaque encrypted data
  });
  
  
  // ========== 5. DECRYPT RESPONSE ==========
  
  const encryptedResponse = await relayResponse.arrayBuffer();
  
  // KMS encrypted response with same HPKE session
  // Decrypt using same symmetric key
  const responseData = await crypto.subtle.decrypt(
    {
      name: "AES-GCM",
      iv: extractNonce(encryptedResponse),
      tagLength: 128
    },
    encryptionKey,
    extractCiphertext(encryptedResponse)
  );
  
  const response = JSON.parse(new TextDecoder().decode(responseData));
  // { wrapped, wrappedKid, receipt }
  
  return response;
}
```

## Operational Excellence Code Examples

### <a id="code-ref-14"></a>Code Reference 14: Ledger Recovery CRD (YAML)


```yaml
# Custom Resource Definition for Ledger Recovery
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: ledgerrecoveries.recovery.accledger.io
spec:
  group: recovery.accledger.io
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          spec:
            type: object
            properties:
              ledgerName:
                type: string
              namespace:
                type: string
              crashDetected:
                type: boolean
              recoveryStrategy:
                type: string
                enum: ["automatic", "manual", "staged"]
          status:
            type: object
            properties:
              phase:
                type: string
                enum: ["Pending", "InProgress", "Completed", "Failed"]
              startTime:
                type: string
                format: date-time
              completionTime:
                type: string
                format: date-time
              message:
                type: string
```

### <a id="code-ref-15"></a>Code Reference 15: Recovery Operator Reconcile Loop (Go)


```go
// Kubernetes Operator with Reconcile loop for disaster recovery
package controller

import (
    "context"
    "time"
    
    recoveryv1 "github.com/azure/accledger/api/v1"
    ctrl "sigs.k8s.io/controller-runtime"
    "sigs.k8s.io/controller-runtime/pkg/client"
    "sigs.k8s.io/controller-runtime/pkg/log"
)

type LedgerRecoveryReconciler struct {
    client.Client
    Scheme  *runtime.Scheme
    Metrics MetricsRecorder
}

// Reconcile is the main reconciliation loop
func (r *LedgerRecoveryReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    log := log.FromContext(ctx)
    startTime := time.Now()
    
    // Fetch the LedgerRecovery resource
    var recovery recoveryv1.LedgerRecovery
    if err := r.Get(ctx, req.NamespacedName, &recovery); err != nil {
        return ctrl.Result{}, client.IgnoreNotFound(err)
    }
    
    // Check current ledger health
    health, err := r.checkLedgerHealth(ctx, recovery.Spec.LedgerName, recovery.Namespace)
    if err != nil {
        return ctrl.Result{RequeueAfter: 30 * time.Second}, err
    }
    
    // If unhealthy and recovery not started, initiate recovery
    if !health.IsHealthy && recovery.Status.Phase == "" {
        log.Info("Starting recovery", "ledger", recovery.Spec.LedgerName, "strategy", recovery.Spec.RecoveryStrategy)
        
        // Update status to InProgress
        recovery.Status.Phase = "InProgress"
        recovery.Status.StartTime = &metav1.Time{Time: time.Now()}
        recovery.Status.Message = "Starting " + recovery.Spec.RecoveryStrategy + " recovery"
        if err := r.Status().Update(ctx, &recovery); err != nil {
            return ctrl.Result{}, err
        }
        
        // Execute recovery strategy
        if err := r.executeRecovery(ctx, &recovery); err != nil {
            recovery.Status.Phase = "Failed"
            recovery.Status.Message = err.Error()
            r.Status().Update(ctx, &recovery)
            return ctrl.Result{}, err
        }
        
        // Update status to Completed
        recovery.Status.Phase = "Completed"
        recovery.Status.CompletionTime = &metav1.Time{Time: time.Now()}
        recovery.Status.Message = "Recovery completed successfully"
        if err := r.Status().Update(ctx, &recovery); err != nil {
            return ctrl.Result{}, err
        }
        
        // Emit metrics
        r.Metrics.RecordRecovery(RecoveryMetrics{
            Ledger:   recovery.Spec.LedgerName,
            Strategy: recovery.Spec.RecoveryStrategy,
            Duration: time.Since(startTime),
        })
    }
    
    // Requeue to continue monitoring
    return ctrl.Result{RequeueAfter: 30 * time.Second}, nil
}

// SetupWithManager sets up the controller with the Manager
func (r *LedgerRecoveryReconciler) SetupWithManager(mgr ctrl.Manager) error {
    return ctrl.NewControllerManagedBy(mgr).
        For(&recoveryv1.LedgerRecovery{}).
        Complete(r)
}
```

### <a id="code-ref-16"></a>Code Reference 16: KEDA ScaledObject for Operator (YAML)


```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: recovery-operator-scaler
  namespace: accledger-system
spec:
  scaleTargetRef:
    name: recovery-operator
  minReplicaCount: 1
  maxReplicaCount: 10
  triggers:
  - type: kubernetes-workload
    metadata:
      podSelector: 'app=recovery-operator'
      value: '5'  # Scale when avg CRD processing queue > 5
```

### <a id="code-ref-17"></a>Code Reference 17: Event-Driven Recovery Configuration (YAML)


```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: ledger-recovery-job
  namespace: accledger-system
spec:
  jobTargetRef:
    parallelism: 1
    completions: 1
    activeDeadlineSeconds: 1800  # 30 minutes
    backoffLimit: 3
    template:
      spec:
        serviceAccountName: recovery-operator
        containers:
        - name: recovery-executor
          image: acr.azurecr.io/ledger-recovery:latest
          env:
          - name: LEDGER_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['ledger-name']
          - name: RECOVERY_STRATEGY
            valueFrom:
              fieldRef:
                fieldPath: metadata.annotations['recovery-strategy']
          resources:
            requests:
              memory: "256Mi"
              cpu: "200m"
            limits:
              memory: "512Mi"
              cpu: "500m"
        restartPolicy: Never
  triggers:
  - type: kubernetes-workload
    metadata:
      podSelector: 'status.phase=Pending,app=ledger-recovery'
      value: '1'  # One job per pending recovery
  maxReplicaCount: 50  # Max 50 concurrent recoveries
  successfulJobsHistoryLimit: 10
  failedJobsHistoryLimit: 10
  scalingStrategy:
    strategy: "accurate"  # Scale based on actual pending recoveries
```

### <a id="code-ref-18"></a>Code Reference 18: Prometheus Monitoring Rules (YAML)


```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ledger-recovery-alerts
spec:
  groups:
  - name: recovery
    interval: 30s
    rules:
    - alert: RecoveryOperatorDown
      expr: up{job="recovery-operator"} == 0
      for: 5m
      annotations:
        summary: "Recovery operator is down"
    - alert: HighRecoveryFailureRate
      expr: rate(recovery_failures_total[5m]) > 0.1
      annotations:
        summary: "High recovery failure rate detected"
```

---

## Impact Summary

### Quantifiable Achievements
- üí∞ **$170K+ total annual cost savings** ($120K disaster recovery + $50K upgrade efficiency)
- ‚ö° **10x performance improvements** across multiple systems
- üìä **99.95%+ uptime** for 500+ production confidential ledgers
- üöÄ **40%+ market expansion** through Python developer enablement
- üîí **First-in-industry** privacy-preserving KMS architecture
---

# By The Numbers

### The Bottom Line
- üí∞ **$170K+ saved annually** ($120K disaster recovery + $50K deployment efficiency)
- ‚ö° **500x faster** deployments (167 hours to 20 minutes)
- üìä **99.95%+ uptime** for 500+ production ledgers globally
- üöÄ **40%+ market expansion** by enabling Python developers
- üîí **First in the industry** with privacy-preserving KMS architecture
- üåç **Global scale** - supporting enterprise customers in every Azure region
- üìâ **99% resource reduction** (256GB ‚Üí 0.5GB, 100 cores ‚Üí 0.2 cores)

### What I Built

1. **CPython in CCF** - Industry first, nobody else has this
2. **OHTTP Privacy Layer** - Regulatory compliance through architectural separation
3. **Kubernetes Operator Pattern** - Now the standard across Azure Confidential Computing
4. **Fleet Operations Optimization** - 10x improvement in deployment speed

### Skills in Action

- **8+ programming languages** across the stack (Go, TypeScript, C#, Python, JavaScript, PowerShell, Bash, YAML)
- **3 cloud platforms** with deep Azure expertise
- **Confidential computing specialization** - TEEs, attestation, cryptographic protocols
- **Production operations** - 500+ services, 99.95%+ SLA, 24/7 on-call responsibility

---
